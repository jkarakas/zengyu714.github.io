---
layout: post
title:  Softmax
date:   2017-03-02 23:13:53 +08:00
categories: DeepLearning
tags:
- DeepLearning
- Softmax
- matplotlib
---

* content
{:toc}


## Original meaning

$$softmax(x_1,...x_n)=log(\sum^n_{i=1}e^{x_i})$$

$$softmax$$ approximates the $$hardmax$$
+ If $$ x_1 > x_2$$, then $$ e^{x1}\ggg e^{x_2}$$, $$e^x$$ **exaggerates** the differences between $$ x_1 $$ and $$ x_2$$.
+ If $$x_1 \ggg x_2$$, $$softmax(x_1, x_2)\leftrightharpoons log(e^{x_1} + e^{x_2}) \thickapprox x_1 \leftrightharpoons max(x_1, x_2)$$ <br><br> e.g. $$x_1 = 3$$, $$x_2 \in (0, 6)$$


{% highlight python %}
import matplotlib.pyplot as plt
import numpy as np
% matplotlib inline
{% endhighlight %}


{% highlight python %}
# hardmax is built-in
hardmax = max

# softmax accpets a vector = {x_1,...x_n}
def softmax(vec):
    vec = np.array(vec, dtype=np.float64)
    return np.log(sum(np.exp(vec)))

# compute the cooresponding maximum
x_coordinate = np.linspace(0, 6, 600)
hard_max = [hardmax(x, 3) for x in x_coordinate]
soft_max = [softmax([x, 3]) for x in x_coordinate]
{% endhighlight %}


{% highlight python %}
plt.figure(figsize=(8, 4))
plt.plot(x_coordinate, hard_max, label='$$max(x, 3)$$')
plt.plot(x_coordinate, soft_max, label='$$log(e^x + e^3)$$')
plt.xlabel('x')
plt.ylabel('maximum')
plt.legend()
{% endhighlight %}




    <matplotlib.legend.Legend at 0x1d0c16a2cf8>




![png](http://wx2.sinaimg.cn/large/9f1c5669gy1fd8wyylb1fj20dx07et8u.jpg)


### Advantages

1. **Smoothness**, it has no sudden change can be differentiated many times.
2. It's convenient to apply **convex optimization**.

### Rescale the softmax
$$\ softmax(x_1,...x_n; k) \equiv \frac{1}{k} log(\sum_{i=1}^n e^{k x_i}) $$<br><br>
As $$k$$ increases, it generates a better approximation of softmax: <br>
however, this comes at a cost of increasing the roughness (size of derivatives) of the function.<br><br>
e.g. $$\ \ \ softmax(1, 2; 1)= 2.3132616875182226$$, <br>
 $$\qquad softmax(1, 2; 10) = 2.0000045398899218$$


{% highlight python %}
def softmax_with_k(vec, k):
    vec_amplified = [i*k for i in vec]
    return softmax(vec_amplified)/k
{% endhighlight %}


{% highlight python %}
x_coordinate = np.linspace(0, 6, 600)
plt.figure(figsize=(10, 8))
plt.axis([2, 4, 2.5, 4.5])

for k in range(1, 11):
    y = [softmax_with_k([x,3], k) for x in x_coordinate]
    plt.plot(x_coordinate, y, label='k = {}'.format(k))
plt.plot(x_coordinate, hard_max, 'k', label='$$max(x, 3)$$')
plt.legend()
plt.xlabel('x')
plt.ylabel('maximum')
{% endhighlight %}




    <matplotlib.text.Text at 0x1d0c174f470>




![png](http://wx4.sinaimg.cn/large/9f1c5669gy1fd8wyz3gdqj20he0dg75d.jpg)


## Machine learning meaning
The **gradient** of the original definition:<br>

$$ \dfrac{\partial {log(\sum^n_{i=1}e^{y_i})}} {\partial y_j} = \dfrac {e^{y_j}} {\sum^n_{i=1}e^{y_i}}$$

$$\Rightarrow \qquad \quad \ \ \  softmax(y_1,...y_n) \equiv (\dfrac {e^{y_1}} {\sum^n_{i=1}e^{y_i}},...,\dfrac {e^{y_n}} {\sum^n_{i=1}e^{y_i}})$$
